{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Pride&Prejudice.txt', 'r') as file:\n",
    "    data_new = file.readlines()\n",
    "\n",
    "data=list(filter('\\n'.__ne__,data_new))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Strip newline character from lines and divide in two sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_1 = []\n",
    "set_2 = []\n",
    "cnt = 0\n",
    "#limit = len(data)/2\n",
    "limit = 5000\n",
    "for line in data: \n",
    "    if(cnt < limit):\n",
    "        set_1.append(line)\n",
    "    else:\n",
    "        set_2.append(line)\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# tokenize and clean the data as in text analytics assignmnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shubhangisrivastava/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_tokens(text):\n",
    "    final_list = []\n",
    "    tokens = nltk.word_tokenize(''.join(text))\n",
    "    \n",
    "    for w in tokens:\n",
    "        if (w.isalpha()):\n",
    "            final_list.append(w.lower())\n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_set_1 = get_tokens(set_1)\n",
    "tokenize_set_2 = get_tokens(set_2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokens are created , now create mapper to create (word , 1) list , write sort method of combined output from two sets , partition method to divide list of words from a-m and n-z , then pass to reducer 1 and 2 to find word counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import re \n",
    "\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "\n",
    "class map_reduce:\n",
    "    def __init__(self , map_fun, red_fun):\n",
    "        self.mapper = map_fun\n",
    "        self.reducer = red_fun\n",
    "    \n",
    "    def merge(self, map_responses):\n",
    "        final = []\n",
    "        for i in map_responses:\n",
    "            final += i\n",
    "        return final\n",
    "        \n",
    "    def sort(self, map_op):\n",
    "        return sorted(map_op, key=lambda map_op_1: map_op_1[0])\n",
    "    \n",
    "    def partition(self, input_list):\n",
    "        list_1 = []\n",
    "        list_2 = []\n",
    "        \n",
    "        for t in input_list:\n",
    "            matches = [s for s in t[0].split() if re.match('^[a-m]', s)]\n",
    "            if matches:\n",
    "                list_1.append(t)\n",
    "            else:\n",
    "                list_2.append(t)\n",
    "        \n",
    "        return list_1, list_2\n",
    "        \n",
    "    def __call__(self,set_1, set_2):\n",
    "        pool = ThreadPool()\n",
    "        output = {}\n",
    "        \n",
    "        map_responses =  pool.map(self.mapper,[set_1, set_2])\n",
    "        combined_result = self.merge(map_responses)\n",
    "        \n",
    "        sorted_result = self.sort(combined_result)\n",
    "        \n",
    "        data_1, data_2 = self.partition(sorted_result)\n",
    "        \n",
    "        red_responses =  pool.map(self.reducer,[data_1, data_2])\n",
    "        \n",
    "        for i in red_responses:\n",
    "            output.update(i)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_it(tokens):\n",
    "    output = []\n",
    "    for token in tokens:\n",
    "        temp = (token , 1)\n",
    "        output.append(temp)\n",
    "    return output\n",
    "\n",
    "def reduce_it(data):\n",
    "    dict_1 = {}\n",
    "    for i in data:\n",
    "        k = i[0]\n",
    "        if k not in dict_1:\n",
    "            dict_1[k] = 0\n",
    "        dict_1[k] += 1\n",
    "    return dict_1\n",
    "\n",
    "m = map_reduce(map_it, reduce_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = m(tokenize_set_1, tokenize_set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Big_Data_Assignment_1_output.csv', 'w') as f:\n",
    "    for i in out.keys():\n",
    "        f.write(\"%s,%s\\n\"%(i,out[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
