{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = './Amazon_Responded_Oct05.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"First App\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Filtering data with user_verified set to FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462030"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sql = SQLContext(sc)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").getOrCreate()\n",
    "\n",
    "\n",
    "#df = spark.read.load(file,format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "df = sql.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', quote='\"', delimiter=',', multiLine = 'true', escape='\"').load(file)\n",
    "#df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\").option(\"escape\", \",\").load(file)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_str: string, tweet_created_at: string, user_verified: boolean, favorite_count: int, retweet_count: double, text_: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = df.select('id_str', 'tweet_created_at','user_verified','favorite_count','retweet_count', 'text_')\n",
    "df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data = df_data[df_data['user_verified'] == \"True\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171899"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.select('text_').distinct().sort('text_').show()\n",
    "filter_data.count()\n",
    "#filter_data.select('user_verified').distinct().sort('user_verified').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf, to_date, to_utc_timestamp\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def get_format(x):\n",
    "    return str(datetime.strptime(x, '%a %b %d %H:%M:%S %z %Y').strftime('%b %d'))\n",
    "\n",
    "date_fn = udf(get_format, StringType())\n",
    "\n",
    "df_test_2 = filter_data.withColumn(\"tweet_created_at\", date_fn(\"tweet_created_at\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id_str=\"'793281386912354304'\", tweet_created_at='Nov 01', user_verified=True, favorite_count=0, retweet_count=0.0, text_=\"@SeanEPanjab I'm sorry, we're unable to DM you. Was this order purchased on https://t.co/nUUp5MLhYl, or one of our other sites? ^CL\"), Row(id_str=\"'793502854459879424'\", tweet_created_at='Nov 01', user_verified=True, favorite_count=0, retweet_count=0.0, text_='@SeanEPanjab Please give us a call/chat so we can look into this order for you: https://t.co/hApLpMlfHN. ^HB')]\n",
      "+----------------+\n",
      "|tweet_created_at|\n",
      "+----------------+\n",
      "|          Nov 27|\n",
      "|          Mar 22|\n",
      "|          Apr 16|\n",
      "|          Apr 27|\n",
      "|          May 24|\n",
      "|          Mar 01|\n",
      "|          Apr 10|\n",
      "|          May 27|\n",
      "|          Nov 01|\n",
      "|          Nov 06|\n",
      "|          Apr 24|\n",
      "|          Dec 10|\n",
      "|          Mar 11|\n",
      "|          May 15|\n",
      "|          May 29|\n",
      "|          Jun 29|\n",
      "|          Jul 12|\n",
      "|          Nov 04|\n",
      "|          Aug 05|\n",
      "|          Jan 21|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_test_2.take(2))\n",
    "\n",
    "df_test_2.select('tweet_created_at').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(tweet_created_at='Nov 27', count=208),\n",
       " Row(tweet_created_at='Mar 22', count=506),\n",
       " Row(tweet_created_at='Apr 16', count=632),\n",
       " Row(tweet_created_at='Apr 27', count=394),\n",
       " Row(tweet_created_at='May 24', count=843),\n",
       " Row(tweet_created_at='Mar 01', count=459),\n",
       " Row(tweet_created_at='Apr 10', count=863),\n",
       " Row(tweet_created_at='May 27', count=95),\n",
       " Row(tweet_created_at='Nov 01', count=370),\n",
       " Row(tweet_created_at='Nov 06', count=357)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GroupBy using dataframe\n",
    "temp = df_test_2.groupby(df_test_2.tweet_created_at).count()\n",
    "temp.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GroupBy and count using RDD object\n",
    "filter_data_rdd = df_test_2.rdd\n",
    "filter_data_rdd.count()\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "grouped_data = filter_data_rdd.map(lambda x: (x[1],1)).groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statement prints key and number of 1's that key occurs\n",
    "\n",
    "#print(list((j[0], list(j[1])) for j in grouped_data.take(5)))\n",
    "freq_tweet = grouped_data.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1536, 'Jan 03'),\n",
       " (1508, 'Jan 10'),\n",
       " (1496, 'Jan 11'),\n",
       " (1410, 'Jan 12'),\n",
       " (1364, 'Jan 06'),\n",
       " (1360, 'Jan 07'),\n",
       " (1342, 'Jan 20'),\n",
       " (1298, 'Mar 02'),\n",
       " (1295, 'Jan 13'),\n",
       " (1292, 'Jan 21'),\n",
       " (1290, 'Jan 14'),\n",
       " (1286, 'Jan 18'),\n",
       " (1279, 'Dec 15'),\n",
       " (1259, 'Jan 24'),\n",
       " (1249, 'Nov 18'),\n",
       " (1201, 'Dec 03'),\n",
       " (1196, 'Jan 02'),\n",
       " (1192, 'Jun 27'),\n",
       " (1190, 'Jul 04'),\n",
       " (1175, 'Jan 19'),\n",
       " (1163, 'Jan 25'),\n",
       " (1149, 'Jan 23'),\n",
       " (1143, 'Jan 08'),\n",
       " (1124, 'Jan 17'),\n",
       " (1120, 'May 11'),\n",
       " (1112, 'Jul 03'),\n",
       " (1109, 'Mar 30'),\n",
       " (1089, 'Apr 05'),\n",
       " (1083, 'Jan 26'),\n",
       " (1080, 'Jan 27'),\n",
       " (1074, 'Jun 20'),\n",
       " (1071, 'Jan 04'),\n",
       " (1058, 'Nov 05'),\n",
       " (1058, 'Feb 22'),\n",
       " (1054, 'Feb 09'),\n",
       " (1045, 'Jun 23'),\n",
       " (1040, 'Jun 30'),\n",
       " (1024, 'Jan 22'),\n",
       " (1020, 'May 03'),\n",
       " (1019, 'Mar 08'),\n",
       " (1019, 'Jul 29'),\n",
       " (1018, 'Mar 29'),\n",
       " (1016, 'Apr 18'),\n",
       " (1007, 'Jan 16'),\n",
       " (1006, 'Jan 15'),\n",
       " (1004, 'Jun 07'),\n",
       " (1003, 'Jan 09'),\n",
       " (987, 'Feb 02'),\n",
       " (982, 'Jun 02'),\n",
       " (979, 'Dec 13'),\n",
       " (979, 'Dec 10'),\n",
       " (978, 'Dec 26'),\n",
       " (968, 'Jan 30'),\n",
       " (968, 'Feb 28'),\n",
       " (960, 'Feb 03'),\n",
       " (956, 'Jan 28'),\n",
       " (955, 'Apr 21'),\n",
       " (955, 'Jun 28'),\n",
       " (953, 'Feb 14'),\n",
       " (952, 'Nov 29'),\n",
       " (940, 'Feb 23'),\n",
       " (926, 'Feb 08'),\n",
       " (925, 'Jul 05'),\n",
       " (924, 'Feb 16'),\n",
       " (921, 'Feb 13'),\n",
       " (921, 'Mar 31'),\n",
       " (920, 'Feb 15'),\n",
       " (918, 'Dec 11'),\n",
       " (914, 'Jan 31'),\n",
       " (907, 'Nov 23'),\n",
       " (906, 'Mar 09'),\n",
       " (902, 'Feb 01'),\n",
       " (898, 'Feb 21'),\n",
       " (884, 'Dec 04'),\n",
       " (880, 'Feb 10'),\n",
       " (878, 'Nov 02'),\n",
       " (878, 'Apr 14'),\n",
       " (876, 'Feb 27'),\n",
       " (875, 'Dec 01'),\n",
       " (875, 'Feb 07'),\n",
       " (872, 'Apr 01'),\n",
       " (869, 'Feb 04'),\n",
       " (868, 'Feb 11'),\n",
       " (864, 'Jul 16'),\n",
       " (863, 'Apr 10'),\n",
       " (843, 'May 24'),\n",
       " (835, 'Dec 31'),\n",
       " (835, 'Feb 24'),\n",
       " (824, 'Dec 05'),\n",
       " (819, 'Dec 21'),\n",
       " (817, 'Jul 19'),\n",
       " (816, 'May 29'),\n",
       " (814, 'Feb 06'),\n",
       " (802, 'May 19'),\n",
       " (801, 'Jun 25'),\n",
       " (798, 'May 13'),\n",
       " (792, 'Jun 13'),\n",
       " (789, 'Nov 20'),\n",
       " (786, 'Feb 25'),\n",
       " (783, 'Jul 11'),\n",
       " (779, 'Feb 20'),\n",
       " (779, 'Jun 11'),\n",
       " (777, 'Apr 02'),\n",
       " (772, 'Jun 15'),\n",
       " (768, 'May 10'),\n",
       " (767, 'Dec 06'),\n",
       " (766, 'Jul 02'),\n",
       " (764, 'Apr 22'),\n",
       " (757, 'Mar 19'),\n",
       " (751, 'Feb 18'),\n",
       " (751, 'Jun 03'),\n",
       " (744, 'Jun 09'),\n",
       " (742, 'Feb 05'),\n",
       " (739, 'Jun 04'),\n",
       " (738, 'Jan 29'),\n",
       " (737, 'Mar 21'),\n",
       " (733, 'Apr 15'),\n",
       " (731, 'Jan 01'),\n",
       " (731, 'Jun 17'),\n",
       " (725, 'Feb 19'),\n",
       " (722, 'Mar 17'),\n",
       " (720, 'Nov 12'),\n",
       " (719, 'Jun 10'),\n",
       " (718, 'Apr 23'),\n",
       " (716, 'Feb 12'),\n",
       " (705, 'May 08'),\n",
       " (700, 'May 22'),\n",
       " (688, 'May 02'),\n",
       " (673, 'Jul 06'),\n",
       " (651, 'Dec 22'),\n",
       " (650, 'Mar 27'),\n",
       " (650, 'May 05'),\n",
       " (650, 'Jun 26'),\n",
       " (647, 'Jun 14'),\n",
       " (646, 'Feb 26'),\n",
       " (634, 'Jul 07'),\n",
       " (632, 'Apr 16'),\n",
       " (627, 'Jul 28'),\n",
       " (615, 'Apr 13'),\n",
       " (613, 'Apr 11'),\n",
       " (593, 'Nov 24'),\n",
       " (592, 'Nov 07'),\n",
       " (590, 'Mar 26'),\n",
       " (587, 'Apr 28'),\n",
       " (563, 'Jun 29'),\n",
       " (558, 'Mar 20'),\n",
       " (555, 'Jul 09'),\n",
       " (543, 'Jul 08'),\n",
       " (541, 'Mar 25'),\n",
       " (533, 'May 25'),\n",
       " (524, 'Mar 03'),\n",
       " (521, 'Nov 26'),\n",
       " (519, 'Nov 08'),\n",
       " (518, 'Dec 29'),\n",
       " (511, 'Apr 19'),\n",
       " (506, 'Mar 22'),\n",
       " (504, 'Mar 28'),\n",
       " (501, 'Jul 13'),\n",
       " (500, 'Jul 14'),\n",
       " (499, 'Apr 09'),\n",
       " (494, 'Jun 05'),\n",
       " (487, 'Mar 13'),\n",
       " (487, 'Jun 16'),\n",
       " (482, 'Jul 22'),\n",
       " (459, 'Mar 01'),\n",
       " (457, 'Nov 19'),\n",
       " (456, 'May 01'),\n",
       " (447, 'Jun 18'),\n",
       " (442, 'Dec 27'),\n",
       " (440, 'Nov 03'),\n",
       " (439, 'Jun 24'),\n",
       " (433, 'Dec 25'),\n",
       " (431, 'Dec 09'),\n",
       " (420, 'Jul 10'),\n",
       " (410, 'Jul 23'),\n",
       " (406, 'Jun 12'),\n",
       " (394, 'Apr 03'),\n",
       " (394, 'Apr 27'),\n",
       " (383, 'May 16'),\n",
       " (374, 'Jul 15'),\n",
       " (373, 'Nov 25'),\n",
       " (372, 'Dec 12'),\n",
       " (370, 'Nov 01'),\n",
       " (367, 'Jun 19'),\n",
       " (363, 'Dec 16'),\n",
       " (362, 'Jul 17'),\n",
       " (360, 'Dec 07'),\n",
       " (357, 'Nov 06'),\n",
       " (356, 'Apr 04'),\n",
       " (355, 'Nov 04'),\n",
       " (349, 'Nov 14'),\n",
       " (343, 'Jun 08'),\n",
       " (342, 'Jul 01'),\n",
       " (342, 'Jul 18'),\n",
       " (337, 'Jan 05'),\n",
       " (332, 'May 18'),\n",
       " (329, 'Mar 14'),\n",
       " (326, 'Mar 18'),\n",
       " (326, 'Mar 24'),\n",
       " (324, 'Mar 10'),\n",
       " (320, 'May 15'),\n",
       " (318, 'Nov 15'),\n",
       " (314, 'Dec 02'),\n",
       " (311, 'May 20'),\n",
       " (306, 'May 17'),\n",
       " (301, 'May 26'),\n",
       " (299, 'Dec 30'),\n",
       " (297, 'Jul 21'),\n",
       " (295, 'Apr 26'),\n",
       " (294, 'Jul 20'),\n",
       " (288, 'Jun 01'),\n",
       " (286, 'Apr 06'),\n",
       " (278, 'Nov 30'),\n",
       " (274, 'Nov 22'),\n",
       " (270, 'Nov 28'),\n",
       " (270, 'Dec 14'),\n",
       " (252, 'Apr 12'),\n",
       " (247, 'Dec 24'),\n",
       " (247, 'Mar 11'),\n",
       " (245, 'Jun 21'),\n",
       " (244, 'May 04'),\n",
       " (244, 'May 12'),\n",
       " (241, 'Mar 06'),\n",
       " (240, 'Dec 19'),\n",
       " (240, 'Jul 31'),\n",
       " (237, 'Apr 30'),\n",
       " (235, 'Jun 06'),\n",
       " (233, 'Apr 24'),\n",
       " (233, 'May 07'),\n",
       " (231, 'Nov 09'),\n",
       " (231, 'May 14'),\n",
       " (228, 'Jul 30'),\n",
       " (226, 'Nov 13'),\n",
       " (221, 'Dec 17'),\n",
       " (220, 'Mar 23'),\n",
       " (216, 'Apr 08'),\n",
       " (215, 'Mar 07'),\n",
       " (213, 'Nov 21'),\n",
       " (208, 'Nov 27'),\n",
       " (207, 'Apr 17'),\n",
       " (204, 'Apr 20'),\n",
       " (203, 'May 06'),\n",
       " (201, 'Mar 16'),\n",
       " (199, 'Jun 22'),\n",
       " (198, 'Apr 07'),\n",
       " (196, 'Dec 23'),\n",
       " (196, 'May 23'),\n",
       " (189, 'May 30'),\n",
       " (187, 'Mar 05'),\n",
       " (178, 'Dec 18'),\n",
       " (175, 'Dec 28'),\n",
       " (163, 'May 31'),\n",
       " (161, 'Jul 12'),\n",
       " (158, 'May 28'),\n",
       " (157, 'Feb 17'),\n",
       " (157, 'Aug 01'),\n",
       " (150, 'Dec 20'),\n",
       " (149, 'Nov 11'),\n",
       " (147, 'May 21'),\n",
       " (125, 'Dec 08'),\n",
       " (106, 'May 09'),\n",
       " (104, 'Nov 16'),\n",
       " (101, 'Nov 17'),\n",
       " (101, 'Mar 12'),\n",
       " (98, 'Mar 04'),\n",
       " (95, 'May 27'),\n",
       " (93, 'Nov 10'),\n",
       " (79, 'Mar 15'),\n",
       " (76, 'Jul 27'),\n",
       " (74, 'Apr 29'),\n",
       " (66, 'Jul 24'),\n",
       " (58, 'Apr 25'),\n",
       " (33, 'Jul 26'),\n",
       " (28, 'Jul 25'),\n",
       " (23, 'Aug 02'),\n",
       " (23, 'Aug 03'),\n",
       " (6, 'Aug 05'),\n",
       " (5, 'Aug 04'),\n",
       " (1, 'Aug 06')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_tweet.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jan 03'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2: calculate the sum of “favorite_count” and “retweet_count” for each tweet on that day \n",
    "#(For the date with highest number of tweets)\n",
    "\n",
    "max_date = freq_tweet.map(lambda x: x[1]).take(1)[0]\n",
    "max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data_max_date = filter_data_rdd.filter(lambda x: x[1] == max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_data_max_df = filter_data_max_date.toDF()\n",
    "filter_data_max_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id_str: string, tweet_created_at: string, user_verified: boolean, favorite_count: bigint, retweet_count: double, text_: string]\n",
      "DataFrame[id_str: string, tweet_created_at: string, user_verified: boolean, favorite_count: int, retweet_count: int, text_: string]\n"
     ]
    }
   ],
   "source": [
    "print(filter_data_max_df)\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "filter_data_max_df = filter_data_max_df.withColumn(\"favorite_count\", filter_data_max_df[\"favorite_count\"].cast(IntegerType()))\n",
    "filter_data_max_df = filter_data_max_df.withColumn(\"retweet_count\", filter_data_max_df[\"retweet_count\"].cast(IntegerType()))\n",
    "\n",
    "print(filter_data_max_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_str: string, tweet_created_at: string, user_verified: boolean, favorite_count: int, retweet_count: int, text_: string, sum_test: int]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_test = filter_data_max_df.withcolumn('sum', reduce(add, ))\n",
    "df_3 = filter_data_max_df.withColumn('sum_test', sum([filter_data_max_df[col] for col in [\"favorite_count\",\"retweet_count\"]]))\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+--------+\n",
      "|favorite_count|retweet_count|sum_test|\n",
      "+--------------+-------------+--------+\n",
      "|             0|            1|       1|\n",
      "|             0|            0|       0|\n",
      "|             1|            0|       1|\n",
      "|             1|            1|       2|\n",
      "|             4|            1|       5|\n",
      "|             2|            1|       3|\n",
      "|             2|            0|       2|\n",
      "+--------------+-------------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_3.take(10)\n",
    "df_3.select('favorite_count','retweet_count', 'sum_test').distinct().show()\n",
    "df_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_str: string, tweet_created_at: string, user_verified: boolean, favorite_count: int, retweet_count: int, text_: string, sum_test: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "sorted_data = df_3.sort(col('sum_test').desc())\n",
    "sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_str: string, text_: string, sum_test: int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_rows = sorted_data.select('id_str', 'text_', 'sum_test').limit(100)\n",
    "top_100_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_str: string, text_: string, sum_test: int, word count: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count words using pyspark dataframe\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "top_100_rows = top_100_rows.withColumn('word count', f.size(f.split(f.col('text_'), ' ')))\n",
    "top_100_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_str=\"'816329761530093568'\", text_='@amazon worst shopping  experience,  no service, no substantial reply to complaints, no delivery for 1 week post guarantee date.', sum_test=5, word count=21),\n",
       " Row(id_str=\"'816083406962434048'\", text_='@ItsJosshA We always aim to deliver by the date given in your confirmation email. Have we missed that date? Any update in tracking?  ^NF', sum_test=3, word count=25),\n",
       " Row(id_str=\"'816086117938319360'\", text_=\"@ItsJosshA Oh no! I'm sorry! Please reach out to us so that we can look into options: https://t.co/hApLpMlfHN. ^JO\", sum_test=2, word count=19),\n",
       " Row(id_str=\"'816095108013654017'\", text_='@KStefl Sounds like you know what to add to your Halloween playlist for this year! ^BV', sum_test=2, word count=16),\n",
       " Row(id_str=\"'816109446069911554'\", text_='@Schoey1992 Happy Birthday, Matt! #FriendsForever #FriendshipGoals ^JO', sum_test=2, word count=7)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_rows.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_rows_rdd_count = top_100_rows.select(\"text_\").rdd.flatMap(lambda x: x[0].split(' ')).map(lambda x: (x, 1)).reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@amazon', 1),\n",
       " ('worst', 1),\n",
       " ('shopping', 3),\n",
       " ('', 9),\n",
       " ('experience,', 1),\n",
       " ('no', 4),\n",
       " ('service,', 1),\n",
       " ('substantial', 1),\n",
       " ('reply', 2),\n",
       " ('to', 62)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_rows_rdd_count.take(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## TASK 2: You will use find_text.csv for this task. There are two columns in this document: “id_str” and “text”. The\n",
    "# second column is empty. Please find out the text content of each tweet according to “id_str” by joining\n",
    "# Amazon_Responded_Oct05.csv and fill in the “text” column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_str: string, text: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_2 = './find_text.csv'\n",
    "\n",
    "write_df = spark.read.load(file_2,format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "write_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53928"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_join = df_data.select('id_str', 'text_')\n",
    "df_2_join = write_df.select('id_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "temp_join = df_1_join.alias('a').join(df_2_join.alias('b'),col('a.id_str') == col('b.id_str'))\n",
    "join_output = temp_join.select([col('a.'+xx) for xx in df_1_join.columns])\n",
    "\n",
    "#+ [col('b.'+xx) for xx in df_2_join.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53927"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_output.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_output.write.csv('./submit_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_rows_rdd_count.toDF().write.csv('./word_freq_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jan 03'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question 2 \n",
    "max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
